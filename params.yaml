LOG_PATH: "logs"
DATA_PATH: "artifacts/data/"
FAISS_DIR: "artifacts/faiss"
CHUNKS_OUT_PATH: "artifacts/faiss/chunks.jsonl"

QUERY_TEXT: "why do we use self-attention heads instead of single attention head in transformers?"

CHUNK_SIZE: 512
CHUNK_OVERLAP: 50
CHUNK_LOWER_LIMIT: 100
CHUNK_UPPER_LIMIT: 500
BATCH_SIZE: 64
INGEST_BATCH_SIZE: 64
K: 10
R: 5
N: 3

ENCODING_MODEL: "cl100k_base"
EMBEDDING_MODEL: "BAAI/bge-small-en"
RE_RANK_MODEL: "cross-encoder/ms-marco-MiniLM-L-6-v2"
LOCAL_LLM: "qwen3:0.6b"